#!/bin/bash


CUDA_VISIBLE_DEVICES=0 DS_SKIP_CUDA_CHECK=1 accelerate launch finetune.py --dataset_name_or_path mmlu --output_dir Llama-2-7b-chat-hf.mmlu.sqalora.lr7e-5.SR01.SW01.SE01.SS01 --logging_strategy steps --logging_steps 1 --save_strategy epoch --dataloader_num_workers 32 --remove_unused_columns False --do_train --ddp_find_unused_parameters False --overwrite_output_dir --bf16 True --tf32 True --max_steps -1 --hard_padding False --save_total_limit 3 --num_train_epochs 3 --learning_rate 7e-5 --per_device_train_batch_size 32 --source_max_len 896 --target_max_len 128 --model_name_or_path /hpc2hdd/home/lzhang330/asset/Llama-2-7b-chat-hf --flash_attn True --report_to wandb --gradient_checkpointing True --peft sqalora --lora_rank 128 --lora_scale 2.0 --sparse True --sparse_ratio 0.1 --sparse_warmup 0.1 --sparse_end 0.1 --sparse_steps 1

CUDA_VISIBLE_DEVICES=0 DS_SKIP_CUDA_CHECK=1 accelerate launch finetune.py --dataset_name_or_path mmlu --output_dir Llama-2-7b-chat-hf.mmlu.sqalora.lr7e-5.SR01.SW01.SE01.SS01.preserve2 --logging_strategy steps --logging_steps 1 --save_strategy epoch --dataloader_num_workers 32 --remove_unused_columns False --do_train --ddp_find_unused_parameters False --overwrite_output_dir --bf16 True --tf32 True --max_steps -1 --hard_padding False --save_total_limit 3 --num_train_epochs 3 --learning_rate 7e-5 --per_device_train_batch_size 32 --source_max_len 896 --target_max_len 128 --model_name_or_path /hpc2hdd/home/lzhang330/asset/Llama-2-7b-chat-hf --flash_attn True --report_to wandb --gradient_checkpointing True --peft sqalora --lora_rank 128 --lora_scale 2.0 --sparse True --sparse_ratio 0.1 --sparse_warmup 0.1 --sparse_end 0.1 --sparse_steps 1 --sparse_preserve_mode 2
